<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>proj | Mu Yang's Website</title><link>https://Mu-Y.github.io/tag/proj/</link><atom:link href="https://Mu-Y.github.io/tag/proj/index.xml" rel="self" type="application/rss+xml"/><description>proj</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 05 Jul 2021 00:00:00 +0000</lastBuildDate><image><url>https://Mu-Y.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>proj</title><link>https://Mu-Y.github.io/tag/proj/</link></image><item><title>Mis-pronunciation Detection based on Phoneme Recognition</title><link>https://Mu-Y.github.io/publication/mpd_sedmdd/</link><pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate><guid>https://Mu-Y.github.io/publication/mpd_sedmdd/</guid><description>&lt;h3 id="method">Method&lt;/h3>
&lt;p>I implemented the system in the &lt;a href="https://ieeexplore.ieee.org/document/9052975" target="_blank" rel="noopener">SED-MDD paper&lt;/a>. Architecture is shown below.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="sed-mdd" srcset="
/publication/mpd_sedmdd/sed-mdd_hu0c9b9a0bfa122cbbbf67f5aec610ece8_106217_90b0efeed957c6975562b67372e55282.webp 400w,
/publication/mpd_sedmdd/sed-mdd_hu0c9b9a0bfa122cbbbf67f5aec610ece8_106217_d5a831b29dbd120faaef799b891ba819.webp 760w,
/publication/mpd_sedmdd/sed-mdd_hu0c9b9a0bfa122cbbbf67f5aec610ece8_106217_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://Mu-Y.github.io/publication/mpd_sedmdd/sed-mdd_hu0c9b9a0bfa122cbbbf67f5aec610ece8_106217_90b0efeed957c6975562b67372e55282.webp"
width="750"
height="457"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Modifications: The original SED-MDD system performs frame-wise phoneme prediction, in a fashion similar to an acoustic model. However I found that the predicted phonemes were quite noisy and could not match the results reported in the paper. Here I&amp;rsquo;m using a &lt;strong>CTC&lt;/strong> loss to replace the original sequence labeling module. Also, I replace the Mel input representations with &lt;a href="https://arxiv.org/abs/2006.11477" target="_blank" rel="noopener">Wav2vec 2.0&lt;/a> representations. The Wav2vec 2.0 representations were extracted from a model which was unsupervisedly pre-trained on Librispeech and fine-tuned on the full 960 hours Librispeech audio.&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>I use &lt;a href="https://github.com/philipperemy/timit" target="_blank" rel="noopener">TIMIT&lt;/a> and &lt;a href="https://psi.engr.tamu.edu/l2-arctic-corpus/" target="_blank" rel="noopener">L2-ARCTIC&lt;/a>, following the train/test split in SED-MDD.&lt;/p>
&lt;h3 id="results-audio-samples">Results (Audio Samples)&lt;/h3>
&lt;p>Take a look at the audio samples below!&lt;/p>
&lt;p>&lt;a href="https://mu-y.github.io/speech_samples/mpd_l2arctic/l2arctic_chinese.html" target="_blank" rel="noopener">Audio samples html page&lt;/a>&lt;/p>
&lt;h3 id="acknowledgement">Acknowledgement&lt;/h3>
&lt;p>This project is a collaboration with &lt;a href="https://psi.engr.tamu.edu/people/shaojin-ding/" target="_blank" rel="noopener">Shaojin Ding&lt;/a>. The code was adapted from Shaojin Ding&amp;rsquo;s initial implementation.&lt;/p></description></item><item><title>Synthing: A WaveNet-based Singing Voice Synthisizer</title><link>https://Mu-Y.github.io/publication/synthsing/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://Mu-Y.github.io/publication/synthsing/</guid><description>&lt;h3 id="method">Method&lt;/h3>
&lt;p>We implemented the Timbre model mentioned in the framework of the &lt;a href="https://www.mdpi.com/2076-3417/7/12/1313" target="_blank" rel="noopener">NPSS&lt;/a> paper.&lt;/p>
&lt;p>Unlike the vanilla sample-to-sample WaveNet, the proposed model makes frame-to-frame predictions on 60-dimensional log-Mel Spectral Frequency Coefficients(&lt;strong>MFSCs&lt;/strong>)and 4-dimensional Band Aperiodicity(&lt;strong>AP&lt;/strong>) Coefficients, using F0(coarse coded), phoneme identity(one-hot coded) and normalized phoneme position(coarse coded) as local control inputs and singer identity as global control inputs. Then we feed generated MFSCs and APs, as well as true F0 into WORLD vocoder to synthesize audio. The features, i.e. MFSCs, APs and F0 are also extracted via WORLD.&lt;/p>
&lt;p>For more details, take a look at our &lt;a href="https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf" target="_blank" rel="noopener">report&lt;/a> and the original &lt;a href="https://www.mdpi.com/2076-3417/7/12/1313" target="_blank" rel="noopener">NPSS paper&lt;/a>.&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>We used two datasets: 1) NIT Japanese Nursery dataset (&lt;a href="http://hts.sp.nitech.ac.jp/archives/2.3/HTSdemo_NIT-SONG070-F001.tar.bz2" target="_blank" rel="noopener">NIT-SONG070-F001&lt;/a>) and 2) self-curated Coldplay songs dataset.&lt;/p>
&lt;h3 id="results-audio-samples">Results (Audio Samples)&lt;/h3>
&lt;p>Listen to some of our sythesized audio samples below!&lt;/p>
&lt;p>&lt;a href="https://mu-y.github.io/speech_samples/synthsing/" target="_blank" rel="noopener">Audio samples html page&lt;/a>&lt;/p></description></item><item><title>Collection and Classification of Lyrics</title><link>https://Mu-Y.github.io/publication/lyrics_classification/</link><pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate><guid>https://Mu-Y.github.io/publication/lyrics_classification/</guid><description>&lt;h3 id="method">Method&lt;/h3>
&lt;h4 id="lyrics-collection">Lyrics Collection&lt;/h4>
&lt;p>We start from a list of artists. We utilize the &lt;a href="https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/iTuneSearchAPI/index.html" target="_blank" rel="noopener">iTunes Search API&lt;/a> to request meta data for each artist, including song name, genre, album/collection name, release year, etc. Then a web crawler is applied to fetch lyrics on &lt;a href="https://genius.com/" target="_blank" rel="noopener">Genius&lt;/a>, using artist names and song names. Finally the collected lyrics are cleaned up by some basic processing: removing too-short lyrics, deduplication, etc.&lt;/p>
&lt;h4 id="classifiers">Classifiers&lt;/h4>
&lt;p>We applied the following classifiers to compare their performances:&lt;/p>
&lt;ul>
&lt;li>KNN, with TF-IDF as features&lt;/li>
&lt;li>Naive Bayes, with TF-IDF as features&lt;/li>
&lt;li>SVM, with TF-IDF as features&lt;/li>
&lt;li>LSTM, with Glove word embedding as features&lt;/li>
&lt;/ul>
&lt;p>For more details, take a look at our &lt;a href="https://github.com/Mu-Y/CSCI544_Prj/blob/master/final_report.pdf" target="_blank" rel="noopener">report&lt;/a>.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>&lt;strong>Lyrics Collection:&lt;/strong> We manage to obtain a lyrics-genre dataset, with 30649 lyrics from 8 genres.&lt;/p>
&lt;p>&lt;strong>Classifiers:&lt;/strong> We train the classifiers on a balanced set - 390 lyrics for each genre, from which we hold out a balanced set as test set. Accuracy on the held-out test set is presented below.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th style="text-align:center">Accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>KNN&lt;/td>
&lt;td style="text-align:center">0.426&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Naive Bayes&lt;/td>
&lt;td style="text-align:center">&lt;strong>0.597&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SVM&lt;/td>
&lt;td style="text-align:center">0.588&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>LSTM&lt;/td>
&lt;td style="text-align:center">0.563&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Interestingly, the Baive Bayes Classifier works pretty well, outperforming all others. That said, I have to recognize, the LSTM model has a few hyperparameters(e.g. hidden layer dimension, learning rate, etc) to be tuned. But due to the time and computation resource limitation, these tuning are not adequately performed. It&amp;rsquo;ll be interesting to see in the future that whether there will be more performance boost with a thorough parameter tuning.&lt;/p></description></item><item><title>Digital Room Correction using Parallel Second-order Filter-based Equalizer</title><link>https://Mu-Y.github.io/publication/roomir-equalizer/</link><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><guid>https://Mu-Y.github.io/publication/roomir-equalizer/</guid><description>&lt;h3 id="method">Method&lt;/h3>
&lt;p>Mostly referred to &lt;a href="https://ieeexplore.ieee.org/document/4529229" target="_blank" rel="noopener">BalÃ¡zs Bank, 2008&lt;/a>&lt;/p>
&lt;h3 id="dataset">Dataset&lt;/h3>
&lt;p>Open-source Room Impulse Response from &lt;a href="http://www.openairlib.net/auralizationdb/content/live-room-sound-studio-laboratory-university-athens" target="_blank" rel="noopener">openairlib.net&lt;/a> (Unfortunately, this amazing website is currently down due to unknown reasons.). The file &lt;code>r8-omni-conf_b.wav&lt;/code> in the &lt;a href="https://github.com/Mu-Y/RoomIR-equalizer.git" target="_blank" rel="noopener">GitHub repo&lt;/a> is one of the recorded Room Impulse Response wav files from a sound studio of the Laboratory of Music Acoustics Technology (LabMAT) at the Department of Music Studies of the University of Athens.&lt;/p>
&lt;h3 id="results-audio-samples">Results (Audio Samples)&lt;/h3>
&lt;p>Listen to the audio samples below!&lt;/p>
&lt;p>&lt;a href="https://mu-y.github.io/speech_samples/roomIR/" target="_blank" rel="noopener">Audio samples html page&lt;/a>&lt;/p></description></item><item><title>Faster-RCNN for Pedestrian Detection in Videos</title><link>https://Mu-Y.github.io/publication/faster_rcnn/</link><pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate><guid>https://Mu-Y.github.io/publication/faster_rcnn/</guid><description>&lt;h3 id="method">Method&lt;/h3>
&lt;p>Re-train &lt;a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">Faster-RCNN&lt;/a> on VOC 2007 and VOC 2012, and also Caltech pedestrian dataset to perform pedestrian detection.&lt;/p>
&lt;p>Apply the model on videos to generate pedestrian bounding-boxes frame-by-frame. Frame-wise bounding boxes are smoothed by moving average. Then the processed frames were concatenated to generate a new video.&lt;/p>
&lt;h3 id="results">Results&lt;/h3>
&lt;p>When fed with a video where pedestrians appear during specific frames, the network will process and output a new video with pedestrians marked by the bouding boxes.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./Picture3.png" alt="picture demo" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p></description></item></channel></rss>