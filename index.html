<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: October 31, 2022 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.d76a0ffe826c7720607759a5005a0aad.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Mu Yang"><meta name=description content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><link rel=alternate hreflang=en-us href=https://mu-y.github.io/><link rel=canonical href=https://mu-y.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#3f51b5"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="twitter:image" content="https://mu-y.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Mu Yang's Website"><meta property="og:url" content="https://mu-y.github.io/"><meta property="og:title" content="Mu Yang's Website"><meta property="og:description" content="A highly-customizable Hugo academic resume theme powered by Wowchemy website builder."><meta property="og:image" content="https://mu-y.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2030-06-01T13:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://mu-y.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://mu-y.github.io/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Mu Yang's Website"><title>Mu Yang's Website</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Mu Yang's Website</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Mu Yang's Website</a></div><div class="navbar-collapse main-menu-item collapse justify-content-center" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#experience data-target=#experience><span>Experience</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#misc data-target=#misc><span>Misc</span></a></li><li class=nav-item><a class=nav-link href=/uploads/cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_hu436c38464c61fd65f6f480eb0ed0bc4c_196329_270x270_fill_q75_lanczos_center.jpg alt="Mu Yang"><div class=portrait-title><h2>Mu Yang</h2></div><ul class=network-icon aria-hidden=true><li><a href=mailto:mu.yang@utdallas.edu aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://twitter.com/MuYang55 target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=vNINDR4AAAAJ&hl=en" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=https://github.com/Mu-Y target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/mu-yang-1678a3131/ target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>About Me</h1><div class=article-style><p>Hi, there! My name is Mu Yang. I&rsquo;m a 2nd year Ph.D. student in Electrical and Computer Engineering (ECE) at University of Texas at Dallas. My supervisor is <a href=https://personal.utdallas.edu/~jxh052100/ target=_blank rel=noopener>Dr. John H. L. Hansen</a>. I&rsquo;m a member of UTD Center for Robust Speech Systems (<a href=https://crss.utdallas.edu/ target=_blank rel=noopener>UTD CRSS</a>). My research interests include Speech Recognition and Speech Synthesis. In the past, I also had experience in Natural Language Processing, especially in Information Extraction (Event and Event Temporal Relation Extraction).</p><p>During my Master&rsquo;s study at USC, I worked with <a href="https://scholar.google.com/citations?user=RKt2sFIAAAAJ&hl=en" target=_blank rel=noopener>Dr. Panayiotis Georgiou</a> on Spoken Language Understanding and Speech Synthesis problems. I also worked with <a href=http://vnpeng.net/ target=_blank rel=noopener>Dr. Nanyun (Violet) Peng</a> on Natural Langauge Processing and Information Extraction at USC Information Scienceses Institute (<a href=https://www.isi.edu/research_groups/nlg/home target=_blank rel=noopener>USC ISI</a>). During 2020-2021, I was a Computer Science Ph.D. student at Texas A&M Univeristy, working on Speech Mis-pronunciation Recognition and Voice/Accent Conversion.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Speech Recognition, Speech Synthesis</li><li>Natural/Spoken Language Processing</li><li>Information Extraction (Event and Event Relation)</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Ph.D. in Electrical and Computer Engineering, 2021-</p><p class=institution>University of Texas at Dallas, U.S.</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Ph.D. in Computer Science, 2020-2021 (quitted)</p><p class=institution>Texas A&M University, U.S.</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>M.S. in Electrical Engineering, 2017-2019</p><p class=institution>University of Southern California, U.S.</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>B.Eng. in Communication Engineering, 2013-2017</p><p class=institution>Chongqing University, China</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Exchange Student, 2016</p><p class=institution>National Sun Yat-sen University, Taiwan</p></div></li></ul></div></div></div></div></div></section><section id=publications class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Publications</h1></div><div class="col-12 col-lg-8"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mu Yang</span>, <span>Andros Tjandra</span>, <span>Chunxi Liu</span>, <span>David Zhang</span>, <span>Duc Le</span>, <span>Ozlem Kalinli</span></span>
(2022).
<a href=/publication/asr_pathways/>Learning ASR pathways: A sparse multilingual ASR model</a>.
<em>submitted to ICASSP 2023</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2209.05735.pdf target=_blank rel=noopener>Preprint</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mu Yang</span>, <span>Kevin Hirschi</span>, <span>Stephen D. Looney</span>, <span>Okim Kang</span>, <span>John H. L. Hansen</span></span>
(2022).
<a href=/publication/mdd/>Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment</a>.
Interspeech 2022 (Oral).<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.isca-speech.org/archive/pdfs/interspeech_2022/yang22v_interspeech.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mu-y.github.io/speech_samples/mdd_IS22/ target=_blank rel=noopener>Audio Samples</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mu Yang</span>, <span>Shaojin Ding</span>, <span>Tianlong Chen</span>, <span>Tong Wang</span>, <span>Zhangyang Wang</span></span>
(2022).
<a href=/publication/lll_tts/>Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis</a>.
ICASSP 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2110.04482.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/lll-tts target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mu-y.github.io/speech_samples/llltts/ target=_blank rel=noopener>Audio Samples</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mu Yang</span>, <span>Darpit Dave</span>, <span>Madhav Erraguntla</span>, <span>Gerard L. Cote</span>, <span>Ricardo Gutierrez-Osuna</span></span>
(2022).
<a href=/publication/hg_prediction/>Joint hypoglycemia prediction and glucose forecasting via deep multi-task learning</a>.
ICASSP 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/document/9746129 target=_blank rel=noopener>PDF</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Mingyu Derek Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jiao Sun</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mu Yang</span>, <span>Kung-Hsiang Huang</span>, <span>Nuan Wen</span>, <span>Shikhar Singh</span>, <span>Rujun Han</span>, <span>Nanyun Peng</span></span>
(2021).
<a href=/publication/eventplus/>EventPlus: A Temporal Event Understanding Pipeline</a>.
NAACL 2021 (Demonstrations).<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2021.naacl-demos.7.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PlusLabNLP/EventPlus target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://kairos-event.isi.edu/ target=_blank rel=noopener>Demo</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Mu Yang</span>, <span>Karolina Nurzynska</span>, <span>Ann E. Walts</span>, <span>Arkadiusz Gertych</span></span>
(2020).
<a href=/publication/cnn_tb/>A CNN-based active learning framework to identify mycobacteria in digitized Ziehl-Neelsen stained human tissues</a>.
Computerized Medical Imaging and Graphics 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.sciencedirect.com/science/article/pii/S0895611120300550 target=_blank rel=noopener>PDF</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Kung-Hsiang Huang</span>, <span class=author-highlighted>Mu Yang</span>, <span>Nanyun Peng</span></span>
(2020).
<a href=/publication/biomed_event_extraction_gnn/>Biomedical Event Extraction with Hierarchical Knowledge Graphs</a>.
EMNLP 2020 (Findings).<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.aclweb.org/anthology/2020.findings-emnlp.114.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PlusLabNLP/GEANet-BioMed-Event-Extraction target=_blank rel=noopener>Code</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Rujun Han</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>I-Hung Hsu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mu Yang</span>, <span>Aram Galstyan</span>, <span>Ralph Weischedel</span>, <span>Nanyun Peng</span></span>
(2019).
<a href=/publication/event_temproal_relation/>Deep Structured Neural Network for Event Temporal Relation Extraction</a>.
CoNLL 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1909.10094.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/PlusLabNLP/Deep-Structured-EveEveTemp target=_blank rel=noopener>Code</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Prashanth G. Shivakumar</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Mu Yang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Panayiotis Georgiou</span></span>
(2019).
<a href=/publication/slu_c2v/>Spoken Language Intent Detection using Confusion2Vec</a>.
InterSpeech 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1904.03576.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/pgurunath/slu_confusion2vec target=_blank rel=noopener>Dataset</a></p></div></div></div></div></section><section id=experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><img src=/media/icons/brands/meta.svg width=56px height=56px alt="Meta AI" loading=lazy></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Intern</div><div class="section-subheading card-title exp-company text-muted my-0">Meta AI</div><div class="text-muted exp-meta">May 2022 –
Aug 2022
<span class=middot-divider></span>
<span>New York City, NY, USA</span></div></div></div><div class=card-text><p>Mentors: Andros Tjandra, Chunxi Liu, David Zhang.</p><p>Develop multilingual ASR technologies for on-device scenario.</p></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><img src=/media/icons/brands/isi.svg width=56px height=56px alt="USC ISI" loading=lazy></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Assistant</div><div class="section-subheading card-title exp-company text-muted my-0">USC ISI</div><div class="text-muted exp-meta">Jul 2019 –
Jul 2020
<span class=middot-divider></span>
<span>Los Angeles, CA, USA</span></div></div></div><div class=card-text><p>Supervisor: Nanyun Peng.</p><p>NLP projects (Information Extraction).</p></div></div></div></div></div></div></div></div></section><section id=featured class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Selected Projects</h1></div><div class="col-12 col-lg-8"><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mu Yang</span></div><span class=article-date>July, 2021</span>
<span class=middot-divider></span>
<span class=pub-publication>Project</span></div><a href=/publication/mpd_sedmdd/><div class=img-hover-zoom><img src=/publication/mpd_sedmdd/featured_hu24d82ed0ebfdf7ddfaed1df54bf6217f_42852_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Mis-pronunciation Detection based on Phoneme Recognition" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/mpd_sedmdd/>Mis-pronunciation Detection based on Phoneme Recognition</a></div><a href=/publication/mpd_sedmdd/ class=summary-link><div class=article-style><p>A Mis-pronunciation Detection system, with word-level aligned phonemes predictions.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mu-y.github.io/speech_samples/mpd_l2arctic/l2arctic_chinese.html target=_blank rel=noopener>Audio Samples</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mu Yang</span>, <span>James Bunning</span>, <span>Shiyu Mou</span>, <span>Sharada Murali</span>, <span>Yixin Yang</span></div><span class=article-date>December, 2018</span>
<span class=middot-divider></span>
<span class=pub-publication><em>USC course EE599: Deep Learning Lab for Speech Processing</em></span></div><a href=/publication/synthsing/><div class=img-hover-zoom><img src=/publication/synthsing/featured_hu4ad38a50654dd665d7d5fb3f5ccd86ff_769967_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Synthing: A WaveNet-based Singing Voice Synthisizer" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/synthsing/>Synthing: A WaveNet-based Singing Voice Synthisizer</a></div><a href=/publication/synthsing/ class=summary-link><div class=article-style><p>Final project for USC course EE599: Deep Learning Lab for Speech Processing - a WaveNet-based singing voice synthesizer. This is a partial implementation of the paper <a href=https://www.mdpi.com/2076-3417/7/12/1313 target=_blank rel=noopener>A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs</a>.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/SynthSing/blob/master/Final_report.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/SynthSing target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/open?id=1LQgP49jjZTb4FVEf5PevsoiBDhCIyaWp" target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mu-y.github.io/speech_samples/synthsing/ target=_blank rel=noopener>Audio Samples</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mu Yang</span>, <span>Tao Chen</span>, <span>Chang Su</span>, <span>Zhe Yang</span></div><span class=article-date>November, 2018</span>
<span class=middot-divider></span>
<span class=pub-publication><em>USC course CSCI544: Applied Natural Language Processing</em></span></div><a href=/publication/lyrics_classification/><div class=img-hover-zoom><img src=/publication/lyrics_classification/featured_hu3d03a01dcc18bc5be0e67db3d8d209a6_7282105_808x455_fill_q75_h2_lanczos_smart1.webp height=455 width=808 class=article-banner alt="Collection and Classification of Lyrics" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/lyrics_classification/>Collection and Classification of Lyrics</a></div><a href=/publication/lyrics_classification/ class=summary-link><div class=article-style><p>Web crawler of lyrics and corresponding music genre. Multiple baseline classifiers, such as Naive Bayes, SVM and Neural Approach(LSTM) are applied to identify the genre of a song by analyzing its lyrics.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/CSCI544_Prj/blob/master/final_report.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/CSCI544_Prj target=_blank rel=noopener>Code</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mu Yang</span></div><span class=article-date>April, 2018</span>
<span class=middot-divider></span>
<span class=pub-publication><em>USC course EE522: Immersive Audio Processing</em></span></div><a href=/publication/roomir-equalizer/><div class=img-hover-zoom><img src=/publication/roomir-equalizer/featured_hu4557b3f1865ecb714bb812c52df220b2_192547_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Digital Room Correction using Parallel Second-order Filter-based Equalizer" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/roomir-equalizer/>Digital Room Correction using Parallel Second-order Filter-based Equalizer</a></div><a href=/publication/roomir-equalizer/ class=summary-link><div class=article-style><p>A Parallel second-order-based equalizer for Room Impulse Response Calibration.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/RoomIR-equalizer/blob/master/final_report.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Mu-Y/RoomIR-equalizer.git target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://mu-y.github.io/speech_samples/roomIR/ target=_blank rel=noopener>Audio Samples</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Mu Yang</span></div><span class=article-date>June, 2017</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Graduation Project for Undergraduates at Chongqing University</em></span></div><a href=/publication/faster_rcnn/><div class=img-hover-zoom><img src=/publication/faster_rcnn/featured_hu21404eab447bd5cf4e04945c18888929_185488_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Faster-RCNN for Pedestrian Detection in Videos" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/faster_rcnn/>Faster-RCNN for Pedestrian Detection in Videos</a></div><a href=/publication/faster_rcnn/ class=summary-link><div class=article-style><p>Train and deploy a Faster-RCNN framework to perform pedestrain detection in videos.</p></div></a></div></div></div></div></section><section id=misc class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Misc.</h1></div><div class="col-12 col-lg-8"><p>I love music! I play guitar (a lot), bass (a little), drum (a little) and violin (>10 years back). I play and sing in a few (unprofessional) bands. We covered songs from our favorite artists. Check out some of our videos!</p><ul><li><a href="https://www.youtube.com/watch?v=-cB48_U216Q" target=_blank rel=noopener>I sing and play guitar</a></li><li><a href="https://www.youtube.com/watch?v=xhl2iMbYU2Q" target=_blank rel=noopener>I sing and play guitar</a> (forgive the poor video quality&mldr;)</li><li><a href="https://www.youtube.com/watch?v=wE7t5uVWLK8" target=_blank rel=noopener>A campus event, 1</a> (forgive the poor video quality&mldr;)</li><li><a href="https://www.youtube.com/watch?v=eZ-lLoLWCkU&t=4s" target=_blank rel=noopener>A campus event, 2</a> (forgive the poor video quality&mldr;)</li></ul></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>