[{"authors":["admin"],"categories":null,"content":"Hi, there! My name is Mu Yang. I‚Äôm a 2nd year Ph.D. student in Electrical and Computer Engineering (ECE) at University of Texas at Dallas. My supervisor is Dr. John H. L. Hansen. I‚Äôm a member of UTD Center for Robust Speech Systems (UTD CRSS). My research interests include Speech Recognition and Speech Synthesis. In the past, I also had experience in Natural Language Processing, especially in Information Extraction (Event and Event Temporal Relation Extraction).\nDuring my Master‚Äôs study at USC, I worked with Dr. Panayiotis Georgiou on Spoken Language Understanding and Speech Synthesis problems. I also worked with Dr. Nanyun (Violet) Peng on Natural Langauge Processing and Information Extraction at USC Information Scienceses Institute (USC ISI). During 2020-2021, I was a Computer Science Ph.D. student at Texas A\u0026amp;M Univeristy, working on Speech Mis-pronunciation Recognition and Voice/Accent Conversion.\n","date":1663113386,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1663113386,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hi, there! My name is Mu Yang. I‚Äôm a 2nd year Ph.D. student in Electrical and Computer Engineering (ECE) at University of Texas at Dallas. My supervisor is Dr. John H.","tags":null,"title":"Mu Yang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://mu-y.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Mu Yang","Andros Tjandra","Chunxi Liu","David Zhang","Duc Le","Ozlem Kalinli"],"categories":[],"content":"","date":1663113386,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663113386,"objectID":"a01aab09cb06adec69fa26dddc53b214","permalink":"https://mu-y.github.io/publication/asr_pathways/","publishdate":"2022-09-13T16:56:26-07:00","relpermalink":"/publication/asr_pathways/","section":"publication","summary":"Neural network pruning can be effectively applied to compress automatic speech recognition (ASR) models. However, in multilingual ASR, performing language-agnostic pruning may lead to severe performance degradation on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks (pathways), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models (-5.0% average WER) and a language-agnostically pruned model (-21.4% average WER), and provide better performance on low-resource languages compared to the monolingual sparse models.","tags":["paper"],"title":"Learning ASR pathways: A sparse multilingual ASR model","type":"publication"},{"authors":["Mu Yang","Kevin Hirschi","Stephen D. Looney","Okim Kang","John H. L. Hansen"],"categories":[],"content":"","date":1652831786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652831786,"objectID":"b2b4ea13b6f941dc30bd5bd467191695","permalink":"https://mu-y.github.io/publication/mdd/","publishdate":"2021-10-09T16:56:26-07:00","relpermalink":"/publication/mdd/","section":"publication","summary":"Current leading mispronunciation detection and diagnosis (MDD) systems achieve promising performance via end-to-end phoneme recognition. One challenge of such end-to-end solutions is the scarcity of human-annotated phonemes on natural L2 speech. In this work, we leverage unlabeled L2 speech via a pseudo-labeling (PL) procedure and extend the fine-tuning approach based on pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec 2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples plus the created pseudo-labeled L2 speech samples. Our pseudo labels are dynamic and are produced by an ensemble of the online model on-the-fly, which ensures that our model is robust to pseudo label noise. We show that fine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and  2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning baseline. The proposed PL method is also shown to outperform conventional offline PL methods. Compared to the state-of-the-art MDD systems, our MDD solution achieves a more accurate and consistent phonetic error diagnosis. In addition, we conduct an open test on a separate UTD-4Accents dataset, where our system recognition outputs show a strong correlation with human perception, based on accentedness and intelligibility.","tags":["paper"],"title":"Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment","type":"publication"},{"authors":["Mu Yang","Shaojin Ding","Tianlong Chen","Tong Wang","Zhangyang Wang"],"categories":[],"content":"","date":1652399786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652399786,"objectID":"c4afe5049bf028bd4c24063553147319","permalink":"https://mu-y.github.io/publication/lll_tts/","publishdate":"2021-10-09T16:56:26-07:00","relpermalink":"/publication/lll_tts/","section":"publication","summary":"This work presents a lifelong learning approach to train a multilingual Text-To-Speech (TTS) system, where each language was seen as an individual task and was learned sequentially and continually. It does not require pooled data from all languages altogether, and thus alleviates the storage and computation burden. One of the challenges of lifelong learning methods is \"catastrophic forgetting\": in TTS scenario it means that model performance quickly degrades on previous languages when adapted to a new language. We approach this problem via a data-replay-based lifelong learning method. We formulate the replay process as a supervised learning problem, and propose a simple yet effective dual-sampler framework to tackle the heavily language-imbalanced training samples. Through objective and subjective evaluations, we show that this supervised learning formulation outperforms other gradient-based and regularization-based lifelong learning methods, achieving 43% Mel-Cepstral Distortion reduction compared to a fine-tuning baseline.","tags":["paper"],"title":"Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis","type":"publication"},{"authors":["Mu Yang","Darpit Dave","Madhav Erraguntla","Gerard L. Cote","Ricardo Gutierrez-Osuna"],"categories":[],"content":"","date":1651967786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651967786,"objectID":"38c564c10116318e9d17e57bec25862f","permalink":"https://mu-y.github.io/publication/hg_prediction/","publishdate":"2021-10-09T16:56:26-07:00","relpermalink":"/publication/hg_prediction/","section":"publication","summary":"We present a multitask learning approach to the problem of hypoglycemia (HG) prediction in diabetes.  The approach is based on a state-of-the-art time series forecasting model, N-BEATS, and extends it by adding a classification task so that the model performs both glucose forecasting (i.e., predicting future glucose values) and HG prediction (i.e., probability of future HG events sometime within the prediction horizon).  We also propose an alternative loss function that penalizes forecasting errors in the HG range.  We evaluate the approach on a dataset containing over 1.6M recordings from 112 patients with type 1 diabetes who wore a continuous glucose monitor (CGM) for 90 days.  Our results show that the classification branch significantly outperforms the forecasting branch on the problem of HG prediction, and that the new loss function is more effective at reducing forecasting errors in the HG range than multi-task learning.","tags":["paper"],"title":"Joint hypoglycemia prediction and glucose forecasting via deep multi-task learning","type":"publication"},{"authors":["Mu Yang"],"categories":null,"content":"Method I implemented the system in the SED-MDD paper. Architecture is shown below.\nModifications: The original SED-MDD system performs frame-wise phoneme prediction, in a fashion similar to an acoustic model. However I found that the predicted phonemes were quite noisy and could not match the results reported in the paper. Here I‚Äôm using a CTC loss to replace the original sequence labeling module. Also, I replace the Mel input representations with Wav2vec 2.0 representations. The Wav2vec 2.0 representations were extracted from a model which was unsupervisedly pre-trained on Librispeech and fine-tuned on the full 960 hours Librispeech audio.\nDataset I use TIMIT and L2-ARCTIC, following the train/test split in SED-MDD.\nResults (Audio Samples) Take a look at the audio samples below!\nAudio samples html page\nAcknowledgement This project is a collaboration with Shaojin Ding. The code was adapted from Shaojin Ding‚Äôs initial implementation.\n","date":1625443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625443200,"objectID":"ac8e011fa8c2834e8167781cf4a807a4","permalink":"https://mu-y.github.io/publication/mpd_sedmdd/","publishdate":"2021-07-05T00:00:00Z","relpermalink":"/publication/mpd_sedmdd/","section":"publication","summary":"A Mis-pronunciation Detection system, with word-level aligned phonemes predictions.","tags":["proj"],"title":"Mis-pronunciation Detection based on Phoneme Recognition","type":"publication"},{"authors":["Mingyu Derek Ma","Jiao Sun","Mu Yang","Kung-Hsiang Huang","Nuan Wen","Shikhar Singh","Rujun Han","Nanyun Peng"],"categories":[],"content":"","date":1610582807,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610582807,"objectID":"8ff3ec6a66eaf219b0a9a5c0c100bea6","permalink":"https://mu-y.github.io/publication/eventplus/","publishdate":"2021-01-13T17:06:47-07:00","relpermalink":"/publication/eventplus/","section":"publication","summary":"We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.","tags":["paper"],"title":"EventPlus: A Temporal Event Understanding Pipeline","type":"publication"},{"authors":["Mu Yang","Karolina Nurzynska","Ann E. Walts","Arkadiusz Gertych"],"categories":[],"content":"","date":1608336407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608336407,"objectID":"f68c69afef825e1b3117f8e66b66a7ef","permalink":"https://mu-y.github.io/publication/cnn_tb/","publishdate":"2020-12-18T17:06:47-07:00","relpermalink":"/publication/cnn_tb/","section":"publication","summary":"Tuberculosis is the most common mycobacterial disease that affects humans worldwide. Rapid and reliable diagnosis of mycobacteria is crucial to identify infected individuals, to initiate and monitor treatment and to minimize or prevent transmission. Microscopic identification of acid-fast mycobacteria (AFB) in tissue sections is usually accomplished by examining Ziehl-Neelsen (ZN) stained slides in which AFB appear bright red against the blue background.\nBecause the ZN-stained slides require time consuming and meticulous screening by an experienced pathologist, our team developed a machine learning pipeline to classify digitized ZN-stained slides as AFB-positive or AFB-negative. The pipeline includes two convolutional neural network (CNN) models to recognize tiles containing AFB, and a logistic regression (LR) model to classify slides based on features from AFB-probability maps assembled from the CNN tile-based classification results. The first CNN was trained using tiles from 6 AFB-positive and 8 AFB-negative slides, and the second CNN was trained using the initial tile set expanded by additional tiles from 19 AFB-negative slides selected within an active learning framework. When evaluated on a separate set of tiles, the two CNNs yielded F1 scores of 99.03% and 98.75%, respectively, and were used to classify tiles in a separate set of 134 slides (46 AFB-positive and 88 AFB-negative). The classification yielded two AFB-probability maps, one for each CNN. The LR model was then 10-fold cross-validated using the average of feature vectors extracted from the AFB-probability maps generated by each CNN. The feature vector consisted of seven features of the AFB-probability map histogram and the positive tile rate (PTR). The sensitivity (87.13%), specificity (87.62%) and F1 (80.18%) achieved by this model were superior to the baseline performance of PTR-based separation of slides that yielded F1 scores of 73.13% and 66.67% in the AFB-probability maps outputted by the CNN trained within the active learning framework and the CNN trained only on the initial set of slides, respectively.\nOur CNNs outperformed several recently published models for AFB detection. Active learning induced robust learning of features by the CNN and led to improved LR classification performance of slides. In the 52 AFB-positive slides used in the pipeline development, the AFB were infrequent, predominantly single and only rarely found in small clusters. Our pipeline can classify slides and visualize suspected AFB-positive areas in each slide, and thus potentially facilitate evaluation of ZN-stained tissue sections for AFB.","tags":["paper"],"title":"A CNN-based active learning framework to identify mycobacteria in digitized Ziehl-Neelsen stained human tissues","type":"publication"},{"authors":["Mu Yang","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://mu-y.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Kung-Hsiang Huang","Mu Yang","Nanyun Peng"],"categories":[],"content":"","date":1605917207,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605917207,"objectID":"113b80d1bcdcc875e5c3855b03cfb5ac","permalink":"https://mu-y.github.io/publication/biomed_event_extraction_gnn/","publishdate":"2020-11-20T17:06:47-07:00","relpermalink":"/publication/biomed_event_extraction_gnn/","section":"publication","summary":"Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus. One of the main challenges is to identify nested structured events that are associated with non-indicative trigger words. We propose to incorporate domain knowledge from Unified Medical Language System (UMLS) to a pre-trained language model via a hierarchical graph representation encoded by a proposed Graph Edgeconditioned Attention Networks (GEANet). To better recognize the trigger words, each sentence is first grounded to a sentence graph based on a jointly modeled hierarchical knowledge graph from UMLS. The grounded graphs are then propagated by GEANet, a novel graph neural networks for enhanced capabilities in inferring complex events. On BioNLP 2011 GENIA Event Extraction task, our approach achieved 1.41% F1 and 3.19% F1 improvements on all events and complex events, respectively. Ablation studies confirm the importance of GEANet and hierarchical KG.","tags":["paper"],"title":"Biomedical Event Extraction with Hierarchical Knowledge Graphs","type":"publication"},{"authors":["Rujun Han","I-Hung Hsu","Mu Yang","Aram Galstyan","Ralph Weischedel","Nanyun Peng"],"categories":[],"content":"","date":1569024407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569024407,"objectID":"5334f3c35534cc87c76f227a8124d603","permalink":"https://mu-y.github.io/publication/event_temproal_relation/","publishdate":"2019-09-20T17:06:47-07:00","relpermalink":"/publication/event_temproal_relation/","section":"publication","summary":"We propose a novel deep structured learning framework for event temporal relation extraction. The model consists of 1) a recurrent neural network (RNN) to learn scoring functions for pair-wise relations, and 2) a structured support vector machine (SSVM) to make joint predictions. The neural network automatically learns representations that account for long-term contexts to provide robust features for the structured model, while the SSVM incorporates domain knowledge such as transitive closure of temporal relations as constraints to make better globally consistent decisions. By jointly training the two components, our model combines the benefits of both data-driven learning and knowledge exploitation. Experimental results on three high-quality event temporal relation datasets (TCR, MATRES, and TB-Dense) demonstrate that incorporated with pre-trained contextualized embeddings, the proposed model achieves significantly better performances than the state-of-the-art methods on all three datasets. We also provide thorough ablation studies to investigate our model.","tags":["paper"],"title":"Deep Structured Neural Network for Event Temporal Relation Extraction","type":"publication"},{"authors":["Prashanth G. Shivakumar","Mu Yang","Panayiotis Georgiou"],"categories":[],"content":"","date":1569023786,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569023786,"objectID":"cb085a144741b242a3281e59841f13b5","permalink":"https://mu-y.github.io/publication/slu_c2v/","publishdate":"2019-09-20T16:56:26-07:00","relpermalink":"/publication/slu_c2v/","section":"publication","summary":"Decoding speaker‚Äôs intent is a crucial part of spoken language understanding (SLU). The presence of noise or errors in the text transcriptions, in real life scenarios make the task more challenging. In this paper, we address the spoken language intent detection under noisy conditions imposed by automatic speech recognition (ASR) systems. We propose to employ confusion2vec word feature representation to compensate for the errors made by ASR and to increase the robustness of the SLU system. The confusion2vec, motivated from human speech production and perception, models acoustic relationships between words in addition to the semantic and syntactic relations of words in human language. We hypothesize that ASR often makes errors relating to acoustically similar words, and the confusion2vec with inherent model of acoustic relationships between words is able to compensate for the errors. We demonstrate through experiments on the ATIS benchmark dataset, the robustness of the proposed model to achieve state-of-the-art results under noisy ASR conditions. Our system reduces classification error rate (CER) by 20.84% and improves robustness by 37.48% (lower CER degradation) relative to the previous stateof-the-art going from clean to noisy transcripts. Improvements are also demonstrated when training the intent detection models on noisy transcripts.","tags":["paper"],"title":"Spoken Language Intent Detection using Confusion2Vec","type":"publication"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you‚Äôll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders ‚Ä¶","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://mu-y.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Mu Yang"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://mu-y.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Mu Yang"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post‚Äôs folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://mu-y.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://mu-y.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Mu Yang","James Bunning","Shiyu Mou","Sharada Murali","Yixin Yang"],"categories":null,"content":"Method We implemented the Timbre model mentioned in the framework of the NPSS paper.\nUnlike the vanilla sample-to-sample WaveNet, the proposed model makes frame-to-frame predictions on 60-dimensional log-Mel Spectral Frequency Coefficients(MFSCs)and 4-dimensional Band Aperiodicity(AP) Coefficients, using F0(coarse coded), phoneme identity(one-hot coded) and normalized phoneme position(coarse coded) as local control inputs and singer identity as global control inputs. Then we feed generated MFSCs and APs, as well as true F0 into WORLD vocoder to synthesize audio. The features, i.e. MFSCs, APs and F0 are also extracted via WORLD.\nFor more details, take a look at our report and the original NPSS paper.\nDataset We used two datasets: 1) NIT Japanese Nursery dataset (NIT-SONG070-F001) and 2) self-curated Coldplay songs dataset.\nResults (Audio Samples) Listen to some of our sythesized audio samples below!\nAudio samples html page\n","date":1543968e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968e3,"objectID":"a486509cfd9f048a6f9c798a35be5b8f","permalink":"https://mu-y.github.io/publication/synthsing/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/synthsing/","section":"publication","summary":"Final project for USC course EE599: Deep Learning Lab for Speech Processing - a WaveNet-based singing voice synthesizer. This is a partial implementation of the paper [A Neural Parametric Singing Synthesizer Modeling Timbre and Expression from Natural Songs](https://www.mdpi.com/2076-3417/7/12/1313).","tags":["proj"],"title":"Synthing: A WaveNet-based Singing Voice Synthisizer","type":"publication"},{"authors":["Mu Yang","Tao Chen","Chang Su","Zhe Yang"],"categories":null,"content":"Method Lyrics Collection We start from a list of artists. We utilize the iTunes Search API to request meta data for each artist, including song name, genre, album/collection name, release year, etc. Then a web crawler is applied to fetch lyrics on Genius, using artist names and song names. Finally the collected lyrics are cleaned up by some basic processing: removing too-short lyrics, deduplication, etc.\nClassifiers We applied the following classifiers to compare their performances:\nKNN, with TF-IDF as features Naive Bayes, with TF-IDF as features SVM, with TF-IDF as features LSTM, with Glove word embedding as features For more details, take a look at our report.\nResults Lyrics Collection: We manage to obtain a lyrics-genre dataset, with 30649 lyrics from 8 genres.\nClassifiers: We train the classifiers on a balanced set - 390 lyrics for each genre, from which we hold out a balanced set as test set. Accuracy on the held-out test set is presented below.\nModel Accuracy KNN 0.426 Naive Bayes 0.597 SVM 0.588 LSTM 0.563 Interestingly, the Baive Bayes Classifier works pretty well, outperforming all others. That said, I have to recognize, the LSTM model has a few hyperparameters(e.g. hidden layer dimension, learning rate, etc) to be tuned. But due to the time and computation resource limitation, these tuning are not adequately performed. It‚Äôll be interesting to see in the future that whether there will be more performance boost with a thorough parameter tuning.\n","date":1542672e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672e3,"objectID":"1bd361dfbffb1dbb300d4c743cf683d2","permalink":"https://mu-y.github.io/publication/lyrics_classification/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/publication/lyrics_classification/","section":"publication","summary":"Web crawler of lyrics and corresponding music genre. Multiple baseline classifiers, such as Naive Bayes, SVM and Neural Approach(LSTM) are applied to identify the genre of a song by analyzing its lyrics.","tags":["proj"],"title":"Collection and Classification of Lyrics","type":"publication"},{"authors":["Mu Yang"],"categories":null,"content":"Method Mostly referred to Bal√°zs Bank, 2008\nDataset Open-source Room Impulse Response from openairlib.net (Unfortunately, this amazing website is currently down due to unknown reasons.). The file r8-omni-conf_b.wav in the GitHub repo is one of the recorded Room Impulse Response wav files from a sound studio of the Laboratory of Music Acoustics Technology (LabMAT) at the Department of Music Studies of the University of Athens.\nResults (Audio Samples) Listen to the audio samples below!\nAudio samples html page\n","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"d24406556236a25a86c61c9520e00a93","permalink":"https://mu-y.github.io/publication/roomir-equalizer/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/publication/roomir-equalizer/","section":"publication","summary":"A Parallel second-order-based equalizer for Room Impulse Response Calibration.","tags":["proj"],"title":"Digital Room Correction using Parallel Second-order Filter-based Equalizer","type":"publication"},{"authors":["Mu Yang"],"categories":null,"content":"Method Re-train Faster-RCNN on VOC 2007 and VOC 2012, and also Caltech pedestrian dataset to perform pedestrian detection.\nApply the model on videos to generate pedestrian bounding-boxes frame-by-frame. Frame-wise bounding boxes are smoothed by moving average. Then the processed frames were concatenated to generate a new video.\nResults When fed with a video where pedestrians appear during specific frames, the network will process and output a new video with pedestrians marked by the bouding boxes.\n","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"3eadc25bfeafeb7eb6dd438c1b9c1785","permalink":"https://mu-y.github.io/publication/faster_rcnn/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/faster_rcnn/","section":"publication","summary":"Train and deploy a Faster-RCNN framework to perform pedestrain detection in videos.","tags":["proj"],"title":"Faster-RCNN for Pedestrian Detection in Videos","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://mu-y.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://mu-y.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Mu Yang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://mu-y.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Mu Yang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://mu-y.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]